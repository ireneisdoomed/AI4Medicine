{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI for Medicine Course 3 Week 1 lecture notebook\n",
    "## Using BioC format and the NegBio Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this lecture notebook! You'll be exploring some of the uses of the `NegBio` library, a tool for biomedical text mining, which you will use in the graded assignment at the end of the week.\n",
    "\n",
    "You'll be using the same dataset as in the assignment, so this is a good opportunity to become more familiar with it. \n",
    "- This dataset consists of 1,000 X-ray reports that have been manually labeled by a board-certified radiologist.\n",
    "- The reports indicate the presence or absence of several different pathologies. \n",
    "- You'll also have access to the extracted \"Report Impression\" section of each report, which is the summary provided for each X-ray. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Pandas and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has shape: (1000, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SimpleTestReportID</th>\n",
       "      <th>Report Impression</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Airspace Opacity</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>Report Impression Parsed</th>\n",
       "      <th>Report Impression DG Paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>\\n \\n1.mild pulmonary edema, and cardiomegaly....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['(S1 (S (S (S (LST (LS 1.)) (NP (JJ mild) (JJ...</td>\n",
       "      <td>['/data3/CXR-CHEST/dgs/GL66832b_GL6dd686/0.pkl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>\\n \\n1.unremarkable cardiomediastinal silhouet...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['(S1 (S (S (NP (LST (LS 1.)) (NN unremarkable...</td>\n",
       "      <td>['/data3/CXR-CHEST/dgs/GL6f51db_GL6f51dd/0.pkl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>\\n1. lines and tubes are unchanged in position...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['(S1 (S (S (LST (LS 1.)) (NP (NP (NNS lines))...</td>\n",
       "      <td>['/data3/CXR-CHEST/dgs/GL666dde_GL6b021a/0.pkl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>\\n1. postoperative portable film with a right-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['(S1 (S (S (LST (LS 1.)) (NP (NP (JJ postoper...</td>\n",
       "      <td>['/data3/CXR-CHEST/dgs/GL6a8d08_GL6d5d21/0.pkl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>\\n \\n1.single frontal view of the chest demons...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['(S1 (S (S (NP (NP (LST (LS 1.)) (JJ single) ...</td>\n",
       "      <td>['/data3/CXR-CHEST/dgs/GL675b56_GL6f4eb9/0.pkl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SimpleTestReportID                                  Report Impression  \\\n",
       "0                 1.0  \\n \\n1.mild pulmonary edema, and cardiomegaly....   \n",
       "1                 2.0  \\n \\n1.unremarkable cardiomediastinal silhouet...   \n",
       "2                 3.0  \\n1. lines and tubes are unchanged in position...   \n",
       "3                 4.0  \\n1. postoperative portable film with a right-...   \n",
       "4                 6.0  \\n \\n1.single frontal view of the chest demons...   \n",
       "\n",
       "   No Finding  Enlarged Cardiomediastinum  Cardiomegaly  Lung Lesion  \\\n",
       "0         NaN                         NaN           1.0          NaN   \n",
       "1         NaN                         0.0           NaN          NaN   \n",
       "2         NaN                         NaN           NaN          NaN   \n",
       "3         NaN                         NaN           NaN          NaN   \n",
       "4         NaN                         NaN           NaN          NaN   \n",
       "\n",
       "   Airspace Opacity  Edema  Consolidation  Pneumonia  Atelectasis  \\\n",
       "0               NaN    1.0           -1.0        NaN          1.0   \n",
       "1               1.0    NaN            0.0       -1.0          NaN   \n",
       "2               1.0    NaN            NaN        NaN          NaN   \n",
       "3               1.0    1.0            NaN        NaN          1.0   \n",
       "4               1.0    NaN            NaN        NaN          NaN   \n",
       "\n",
       "   Pneumothorax  Pleural Effusion  Pleural Other  Fracture  Support Devices  \\\n",
       "0           NaN               1.0            NaN       NaN              1.0   \n",
       "1           0.0               0.0            NaN       1.0              NaN   \n",
       "2          -1.0               NaN            NaN       NaN              1.0   \n",
       "3           1.0               NaN            NaN       NaN              1.0   \n",
       "4           0.0               1.0            NaN       NaN              1.0   \n",
       "\n",
       "                            Report Impression Parsed  \\\n",
       "0  ['(S1 (S (S (S (LST (LS 1.)) (NP (JJ mild) (JJ...   \n",
       "1  ['(S1 (S (S (NP (LST (LS 1.)) (NN unremarkable...   \n",
       "2  ['(S1 (S (S (LST (LS 1.)) (NP (NP (NNS lines))...   \n",
       "3  ['(S1 (S (S (LST (LS 1.)) (NP (NP (JJ postoper...   \n",
       "4  ['(S1 (S (S (NP (NP (LST (LS 1.)) (JJ single) ...   \n",
       "\n",
       "                          Report Impression DG Paths  \n",
       "0  ['/data3/CXR-CHEST/dgs/GL66832b_GL6dd686/0.pkl...  \n",
       "1  ['/data3/CXR-CHEST/dgs/GL6f51db_GL6f51dd/0.pkl...  \n",
       "2  ['/data3/CXR-CHEST/dgs/GL666dde_GL6b021a/0.pkl...  \n",
       "3  ['/data3/CXR-CHEST/dgs/GL6a8d08_GL6d5d21/0.pkl...  \n",
       "4  ['/data3/CXR-CHEST/dgs/GL675b56_GL6f4eb9/0.pkl...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from file\n",
    "df = pd.read_csv(\"stanford_report_test.csv\")\n",
    "\n",
    "# Check the num of rows, columns\n",
    "print(f\"dataset has shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################\n",
      "Report number: 1\n",
      "\n",
      " \n",
      "1.mild pulmonary edema, and cardiomegaly.  trace pleural fluid \n",
      "effusions.\n",
      " \n",
      "2.low lung volumes with minimal basilar atelectasis.\n",
      " \n",
      "3.no new focal consolidation. \n",
      " \n",
      "4.interval placement of defibrillation pads.  \n",
      " \n",
      "\n",
      "################################\n",
      "Report number: 2\n",
      "\n",
      " \n",
      "1.unremarkable cardiomediastinal silhouette\n",
      " \n",
      "2.diffuse reticular pattern, which can be seen with an atypical \n",
      "infection or chronic fibrotic change.  no focal consolidation.\n",
      " \n",
      "3.no pleural effusion or pneumothorax\n",
      " \n",
      "4.mild degenerative changes in the lumbar spine and old right rib \n",
      "fractures. \n",
      " \n",
      "\n",
      "################################\n",
      "Report number: 3\n",
      "\n",
      "1. lines and tubes are unchanged in position.\n",
      "2. increasing retrocardiac opacity and left midlung zone opacity.\n",
      "3. there is a deep left costophrenic sulcus which is increased when\n",
      "compared with prior films. no definite evidence of left\n",
      "pneumothorax. clinical correlation is recommended. if clinically\n",
      "indicated, consider film in expiration or decubitus views.\n",
      "4. the icu team was informed of these results at 10 am on 05_02_2005.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a better view of the report impression column\n",
    "for i in range(3):\n",
    "    print(\"################################\")\n",
    "    print(f\"Report number: {i+1}\")\n",
    "    print(df.loc[i, 'Report Impression'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing BioC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started by looking at the `BioC` module. You'll be using `BioC` to convert your clinical data into a standard format that can be leveraged on more specialized libraries. This module is used for many other NLP tasks as well, such as serialization or deserialization of data. You can read more about it [here](http://bioc.sourceforge.net/).\n",
    "\n",
    "For your purposes, you're interested in the `BioCCollection` object, which represents a collection of documents for a project. The collection might be an entire corpus, or a partial one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributes with value: \n",
      "\n",
      "{'encoding': 'utf-8', 'version': '1.0', 'standalone': True, 'source': '', 'date': '2020-07-05', 'key': '', 'infons': {}, 'documents': []}\n",
      "\n",
      "methods and attributes: \n",
      "\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'add_document', 'clear_infons', 'date', 'documents', 'encoding', 'infons', 'key', 'source', 'standalone', 'version']\n",
      "\n",
      "documents within collection: []\n"
     ]
    }
   ],
   "source": [
    "import bioc\n",
    "\n",
    "collection = bioc.BioCCollection()\n",
    "print(f\"attributes with value: \\n\\n{collection.__dict__}\\n\")\n",
    "print(f\"methods and attributes: \\n\\n{dir(collection)}\\n\")\n",
    "print(f\"documents within collection: {collection.documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Text for BioC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with collections, you're mostly interested in the documents attribute and the `add_document()` method.\n",
    "\n",
    "The `BioC` module gives you a standard format that allows you to apply other, more specialized libraries. Before seeing `BioC` in action, let's introduce `NegBio`, a tool that distinguishes negative or uncertain findings in radiology reports. It accomplishes this by using patterns on universal dependencies, instead of using rule-based methods. If you'd like to know more, check out the official github [repo](https://github.com/ncbi-nlp/NegBio), or the official [documentation](https://negbio.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "You'll be using the `NegBioSSplitter` object to split your text into sentences. However, in order to do this, you'll first need to convert your text into a format that `BioC` supports. For this you'll use the `text2bioc()` function, which transforms the text into a `BioC` XML file. You can go even further and convert the text into documents with the `text2document()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from negbio.pipeline.ssplit import NegBioSSplitter\n",
    "from negbio.pipeline import text2bioc\n",
    "\n",
    "splitter = NegBioSSplitter()\n",
    "for i, report in enumerate(df[\"Report Impression\"]):\n",
    "        document = text2bioc.text2document(str(i), report) # Texto a objeto BioC\n",
    "        document = splitter.split_doc(document) # Documento BioC lo separa por frases\n",
    "        collection.add_document(document) # Añade cada documento separado a la colección BioC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n \\n1.mild pulmonary edema, and cardiomegaly.  trace pleural fluid \\neffusions.\\n \\n2.low lung volumes with minimal basilar atelectasis.\\n \\n3.no new focal consolidation. \\n \\n4.interval placement of defibrillation pads.  \\n \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Report Impression\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioCDocument[id=0,infons=[],passages=[BioCPassage[offset=0,text='\\n \\n1.mild pulmona ... lation pads.  \\n \\n',infons=[],sentences=[BioCSentence[offset=0,text='\\n \\n1.mild pulmona ... and cardiomegaly.',infons=[],annotations=[],relations=[],],BioCSentence[offset=46,text='trace pleural fluid \\neffusions.',infons=[],annotations=[],relations=[],],BioCSentence[offset=80,text='2.low lung volume ... ilar atelectasis.',infons=[],annotations=[],relations=[],],BioCSentence[offset=135,text='3.no new focal consolidation.',infons=[],annotations=[],relations=[],],BioCSentence[offset=168,text='4.interval placem ... ibrillation pads.',infons=[],annotations=[],relations=[],]],annotations=[],relations=[],]],annotations=[],relations=[],]\n",
      "<class 'bioc.bioc.BioCDocument'>\n"
     ]
    }
   ],
   "source": [
    "print(collection.documents[0])\n",
    "print(type(collection.documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Documents\n",
    "\n",
    "Now your `BioC` collection has been filled with documents, but the output is very hard to read. Let's break it down a little more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collection.documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like you have a document for each report impression. But what's stored inside each document? Let's check the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioCDocument[id=0,infons=[],passages=[BioCPassage[offset=0,text='\\n \\n1.mild pulmona ... lation pads.  \\n \\n',infons=[],sentences=[BioCSentence[offset=0,text='\\n \\n1.mild pulmona ... and cardiomegaly.',infons=[],annotations=[],relations=[],],BioCSentence[offset=46,text='trace pleural fluid \\neffusions.',infons=[],annotations=[],relations=[],],BioCSentence[offset=80,text='2.low lung volume ... ilar atelectasis.',infons=[],annotations=[],relations=[],],BioCSentence[offset=135,text='3.no new focal consolidation.',infons=[],annotations=[],relations=[],],BioCSentence[offset=168,text='4.interval placem ... ibrillation pads.',infons=[],annotations=[],relations=[],]],annotations=[],relations=[],]],annotations=[],relations=[],]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document has an attribute called \"passages\" in which the sentences are stored. Notice that `passages` is a list, but for this case it will only have one element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BioCSentence[offset=0,text='\\n \\n1.mild pulmona ... and cardiomegaly.',infons=[],annotations=[],relations=[],],\n",
       " BioCSentence[offset=46,text='trace pleural fluid \\neffusions.',infons=[],annotations=[],relations=[],],\n",
       " BioCSentence[offset=80,text='2.low lung volume ... ilar atelectasis.',infons=[],annotations=[],relations=[],],\n",
       " BioCSentence[offset=135,text='3.no new focal consolidation.',infons=[],annotations=[],relations=[],],\n",
       " BioCSentence[offset=168,text='4.interval placem ... ibrillation pads.',infons=[],annotations=[],relations=[],]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.documents[0].passages[0].sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence stores information about the text, offset, relations and annotations. Let's check the sentences saved in the first document of our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence number 1: \n",
      " \n",
      "1.mild pulmonary edema, and cardiomegaly.\n",
      "\n",
      "###############################\n",
      "\n",
      "sentence number 2: trace pleural fluid \n",
      "effusions.\n",
      "\n",
      "###############################\n",
      "\n",
      "sentence number 3: 2.low lung volumes with minimal basilar atelectasis.\n",
      "\n",
      "###############################\n",
      "\n",
      "sentence number 4: 3.no new focal consolidation.\n",
      "\n",
      "###############################\n",
      "\n",
      "sentence number 5: 4.interval placement of defibrillation pads.\n",
      "\n",
      "###############################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,s in enumerate(collection.documents[0].passages[0].sentences):\n",
    "    print(f\"sentence number {i + 1}: {s.text}\\n\")\n",
    "    print(\"###############################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up with the clean() function\n",
    "\n",
    "Notice how the first report impression, which had two sentences, was split successfully. However, the newlines have not been trimmed. The `clean()` function from the previous lecture notebook will come in handy here. Let's bring it back out of the toolbox and apply it in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(sentence):\n",
    "    lower_sentence = sentence.lower()\n",
    "    corrected_sentence = re.sub('and/or', 'or', lower_sentence)\n",
    "    corrected_sentence = re.sub('(?<=[a-zA-Z])/(?=[a-zA-Z])', ' or ', corrected_sentence)\n",
    "    clean_sentence = corrected_sentence.replace(\"..\", \".\")\n",
    "    punctuation_spacer = str.maketrans({key: f\"{key} \" for key in \".,\"})\n",
    "    clean_sentence = clean_sentence.translate(punctuation_spacer)\n",
    "    clean_sentence = ' '.join(clean_sentence.split())\n",
    "    return clean_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've spent some time exploring how the `NegBio` library works, let's try it out on your data. \n",
    "\n",
    "You'll determine whether a given report impression can tell you if a patient has an existing condition, while taking into account whether there was negation or uncertainty in the findings. For this task, you'll use these predetermined categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"Cardiomegaly\", \"Lung Lesion\", \"Airspace Opacity\", \"Edema\", \n",
    "              \"Consolidation\", \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\", \n",
    "              \"Pleural Effusion\", \"Pleural Other\", \"Fracture\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import NegBio Dependencies\n",
    "\n",
    "Next you'll import everything you need for this task. Don't be alarmed by the declared paths below the imports! They're just mapping the path to various files that `NegBio` relies on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib2 import Path\n",
    "from negbio.main_chexpert import pipeline\n",
    "from negbio.pipeline.parse import NegBioParser\n",
    "from negbio.chexpert.stages.load import NegBioLoader\n",
    "from negbio.chexpert.stages.extract import NegBioExtractor\n",
    "from negbio.chexpert.stages.classify import ModifiedDetector\n",
    "from negbio.chexpert.stages.aggregate import NegBioAggregator\n",
    "from negbio.pipeline.ptb2ud import NegBioPtb2DepConverter, Lemmatizer\n",
    "\n",
    "PARSING_MODEL_DIR = \"~/.local/share/bllipparser/GENIA+PubMed\"\n",
    "CHEXPERT_PATH = \"NegBio/negbio/chexpert/\"\n",
    "MENTION_PATH =f\"{CHEXPERT_PATH}phrases/mention\"\n",
    "UNMENTION_PATH = f\"{CHEXPERT_PATH}phrases/\"\n",
    "NEG_PATH = f'{CHEXPERT_PATH}patterns/negation.txt'\n",
    "PRE_NEG_PATH = f'{CHEXPERT_PATH}patterns/pre_negation_uncertainty.txt'\n",
    "POST_NEG_PATH = f'{CHEXPERT_PATH}patterns/post_negation_uncertainty.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoding of information within these files is beyond the scope of this notebook, but if you're really curious about the contents you could do something like this to see more: \n",
    "```python\n",
    "!cat $NEG_PATH\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $NEG_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this process for the entire dataset is very slow (~1.5 hr on a fast laptop!) so let's slice it to showcase how `NegBio` works. Let's start with 50 random observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = df.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's recreate the code from the beginning of the notebook as a function, including the `clean()` function as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bioc_collection(df):\n",
    "    collection = bioc.BioCCollection()\n",
    "    splitter = NegBioSSplitter()\n",
    "    for i, report in enumerate(df[\"Report Impression\"]):\n",
    "        document = text2bioc.text2document(str(i), clean(report))\n",
    "        document = splitter.split_doc(document)\n",
    "        collection.add_document(document)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll repeat your process from earlier by converting the report impression strings into a `BioC` XML format which `NegBio` can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = get_bioc_collection(sampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate `NegBio`'s lemmatizer. \n",
    "\n",
    "The process of lemmatization refers to returning the dictionary form of a word (or lemma) by removing inflectional endings. It's very cool and you can read more about it [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you'll instantiate `NegBio`'s converter to convert from parse tree to universal dependencies. This is done using the Stanford converter, which you can find more information about [here](https://github.com/dmcc/PyStanfordDependencies).\n",
    "\n",
    "The parse tree used here is the [Penn Treebank](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html). In general terms, a treebank is an annotated text corpus that includes analysis beyond part-of-speech tagging. They've become very valuable resources to NLP research in recent years.\n",
    "\n",
    "Universal dependencies, or UD, provide a powerful framework for annotating grammar across different languages. Read more about them [here](https://universaldependencies.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb2dep = NegBioPtb2DepConverter(lemmatizer, universal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've already seen the splitter in action before, so you can skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssplitter = NegBioSSplitter(newline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll instantiate the parser and the loader. \n",
    "\n",
    "Under the hood, you're using the [BLIPP reranking parser](https://github.com/BLLIP/bllip-parser), which is a statistical natural language parser. \n",
    "\n",
    "The loader, as you might imagine, loads the reports into memory.\n",
    "\n",
    "Over all of this, the [chexpert-labeler](https://github.com/stanfordmlgroup/chexpert-labeler) is used. This labeler extracts observations from radiology reports specifically, and can provide a vocabulary appropriate to the clinical context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = NegBioParser(model_dir=PARSING_MODEL_DIR)\n",
    "loader = NegBioLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extractor is what extracts the observations from the report impressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = NegBioExtractor(Path(MENTION_PATH), Path(UNMENTION_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negator will determine whether negation or uncertainty exists in the context of the observations provided by the extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_detector = ModifiedDetector(PRE_NEG_PATH, NEG_PATH, POST_NEG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregator then aggregates these observations if they belong to the same category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregator = NegBioAggregator(CATEGORIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you'll put everything together using the pipeline function, which takes as arguments all of the objects you've instantiated so far. Then you'll get a nice, clean DataFrame with your result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:59<00:00,  1.19s/it]\n"
     ]
    }
   ],
   "source": [
    "collection = pipeline(collection, loader, ssplitter, extractor, \n",
    "                          parser, ptb2dep, neg_detector, aggregator, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "negbio_pred = pd.DataFrame()\n",
    "for doc in collection.documents:\n",
    "    dictionary = {}\n",
    "    for key, val in doc.infons.items():\n",
    "        dictionary[key[9:]] = val\n",
    "    negbio_pred = negbio_pred.append(dictionary, ignore_index=True)\n",
    "negbio_pred = negbio_pred.replace(\n",
    "    \"Positive\", True).replace(\n",
    "    \"Negative\", False).replace(\"Uncertain\", False).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airspace Opacity</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Lung Lesion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Airspace Opacity  Pneumothorax  Atelectasis  Cardiomegaly  Consolidation  \\\n",
       "0              True         False        False         False          False   \n",
       "1              True         False         True         False          False   \n",
       "2             False         False        False         False          False   \n",
       "3              True         False        False         False          False   \n",
       "4             False         False        False         False          False   \n",
       "\n",
       "   Pleural Effusion  Pneumonia  Fracture  Pleural Other  Edema  Lung Lesion  \n",
       "0             False      False     False          False  False        False  \n",
       "1             False      False     False          False  False        False  \n",
       "2             False      False     False          False  False        False  \n",
       "3             False      False     False          False  False        False  \n",
       "4             False      False      True           True  False        False  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(negbio_pred.shape)\n",
    "negbio_pred.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can check every entry in the report impressions for the presence of a condition, while knowing that negation has been taken into account. Really cool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this notebook!!!** This was a very high-level explanation of everything that NegBio does and as you may have noticed, this library leverages many other great tools and libraries. Hopefully, it was a good introduction to how it works. **Nice work, keep it up!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
